% Lokální makra patří do hlavního souboru, ne sem.
% Tady je mám výjimečně proto, že chci nechat hlavní soubor bez maker,
% která jsou jen pro tento dokument. Uživatelé si pravděpodobně budou
% hlavní soubor kopírovat do svého dokumentu.

\def\ctustyle{{\ssr CTUstyle}}
\def\ttb{\tt\char`\\} % pro tisk kontrolních sekvencí v tabulkách

\label[ProblematicResearch]
\chap Problematic research

In this chapter, I studied the standards for design and implementation of software for fail-safe platforms in railway infrastructure environments. I have tried to identify the differences between the different safety integration levels and discuss the specific mechanisms that lead to meeting the safety requirements given by the standards.

\label[SafetySecurity]
\sec The difference between security and safety

Before studying the issue, it is necessary to get the terminology right. In the English language, there are two words: safety and security.

The word {\sbf security} refers to the discipline that aims to protect against harmful attacks or actions such as vandalism, cyber-attack, or terrorism. Physical barriers, authentication, encryption, or monitoring systems are used to protect.~\cite[6i2KXzgcdbBD85cySecurity]

The word {\sbf safety} refers to the discipline that aims to protect against accidents, errors, or technical failures that can lead to injury, loss of life, or damage to property. Such typical threats are natural hazards, technical failures, or human factors, i.e. human failure.~\cite[6i2KXzgcdbBD85cySafety]


\midinsert \clabel[SafetyvsSecurity]{Main differences between safety and security}
\ctable{lll}{
\hfil {\sbf Aspect} & {\sbf Safety} & {\sbf Security} \crl
Focus & accident prevention & prevention of deliberate attacks \cr
& and technical failures & \cr
\cr
Threats & natural hazards, errors, & human intentional actions,\cr
& technical failures & cyber attacks \cr
\cr
Access & system reliability, & security, encryption, \cr
& standards & monitoring \cr
\cr
Example & fail-safe systems, & physical security, \cr
of measures & resilience testing & cybersecurity systems \cr
}
\caption/t Main differences between the terms safety and security
\endinsert

Table \ref[SafetyvsSecurity] compares the different aspects of safety and security in order to highlight the different meanings. In this thesis, I will deal primarily with the second concept - safety.

\sec Materials and standards

The safety requirements for transport infrastructure in the European Union are defined in standards issued by CENELEC.\fnote{European Committee for Electrotechnical Standardization} These standards are {\it EN 50126}, {\it EN 50128}, {\it EN 50129}, and {\it EN 50159}.

The standard {\sbf EN 50126} relates to railway equipment in general and specifies and defines how to demonstrate reliability, availability, maintainability and safety (RAMS). For more information see chapter \ref[RAMS].

The standard {\sbf EN 50129} describes the standard for the railway communication and signaling system and the data processing system (software). The annexes provide detailed instructions on how to test hardware components including microcontroller components. The standard {\sbf EN 50159} complements the standard for secure communications.

Standard {\sbf EN 50128} defines a standard on how to properly develop secure software for railway infrastructure.

\label[RAMS]
\sec RAMS

The acronym RAMS is composed of the initial letters of the words {\sbf Reliability} (components do not fail too often), {\sbf Availability} (to be sure that components will work as required and that if they fail, they will not affect the functionality of the equipment), {\sbf Maintainability} (and easy replaceability in the event of damage), and {\sbf Safety} (described in the Chap \ref[SafetySecurity]).

We are never able to achieve 100\% success in any of the RAMS aspects, we are always only approaching it. The level at which we want to achieve it needs to be defined.

It is necessary to remember the economic side of the issue. The ratio of the investment in making the platform safe and how much it will cost to develop and produce is growing exponentially, in general. Thus, going from 20\% to 60\% security will be much less economically challenging than going from 99.99\% to 99.999999\%. If we develop this theory into implications, we necessarily arrive at the question - how expensive is human life? Theoretically, this cost could be quantified according to standards, state-specific requirements, and crash statistics.

If we go back to all four aspects, we find that some of the essences are mutually defining. For example, we're going to make the equipment more secure and more robust; it's going to complicate the ease of replacement.

I also see it important to answer the question - why is it necessary to pay attention to system availability or repairability? After all, it is only the security itself that is important to us. If we think about it, as a result, all the aspects already mentioned are important because they complement each other. You cannot achieve safety without having a low failure rate system, or you cannot achieve safety without a possible system failure affecting the requirement that the component is supposed to fulfill.

\sec System and random failures

{\sbf Random failures} can be described by statistical distributions.~\cite[GaaBVFT5zA4FlnK8] We cannot control failures, we can only predict and model them mathematically. In most cases, the failure is due to a physical cause such as material fatigue, wear, corrosion, or random failures of electronic components.

The {\it EN 50126} standard defines {\sbf system failures} as failures caused by errors in system life cycle activities that lead to deterministic failure of a product, system, or process under certain combinations of inputs or conditions.~\cite[GaaBVFT5zA4FlnK8] Unlike random failures, system failures are usually caused by human errors at various stages of the system life cycle, namely specification, system design, development, manufacture, or installation. These failures, on the other hand, are preventable from accidental failures by process, testing, and validation phases.

By the norm {\it EN 50129}, these errors are, for example: specification errors, design errors, manufacturing errors, installation errors, operation errors, maintenance errors, or modification errors.

\midinsert \clabel[RandomVsSystematic]{Main differences between random and systematic failure}
\ctable{lll}{
\hfil {\sbf Aspect} & {\sbf Random failure} & {\sbf Systematic failure}\crl
Cause & physical processes & human error during design, \cr
 & (fatigue, wear, environment) & implementation or maintenance \cr
\cr
Characteristic & stochastic, unpredictable & deterministic, predictable \cr
 & at the individual level & under certain conditions \cr
\cr
Solution & predictive maintenance, & validation, verification, \cr
 & statistical analysis, redundancy & process measures \cr
\cr
Repetition & not repeatable under & repeated every time \cr
 & the same conditions & under the same conditions \cr
\cr
Examples & rail breakage, & software error, \cr
 & bearing failure & sensor miscalibration \cr
}
\caption/t Main differences between random and systematic failures.  
\endinsert

\label[RAMSCycle]
\secc RAMS development cycle

{\sbf The RAMS development cycle} is described by the V-model, that uses {\it top-down} and {\it bottom-up} approaches.~\cite[SU4ccYhZgv796Fi2] It is a graphical representation of the complete development lifecycle.\cite[L9XZRy6g04sicEwo] The advantages of this model are a clear structure, easy traceability between the design, respective requirements, and testing results. As a result, the use of this model enables early identification of problems.

The model also has many disadvantages such as the lack of flexibility; the later a bug is discovered, the more expensive it is to fix.\cite[1fwla6zm8H5QiWZn] However, in the context of rail transport, these weaknesses are a necessity to ensure our safety.

{\it EN 50128} describes in more detail the different phases and roles that are responsible for each phase. The V-model is specific to two activities - validation and verification.

\medskip
\clabel[VModel]{V-model illustration}
\picw=14cm \cinspic img/02-v-model.png
\caption/f V-model illustration by {\it EN 50128} norm definition.~\cite[vDBnVxdnHZs4vPl6]
\medskip

\seccc Verification and validation

The standard defines {\sbf verification} as the confirmation, through provided objective evidence, that defined requirements have been met and {\sbf validation} as the confirmation, through provided objective evidence, that requirements for a specific purpose or application have been met.~\cite[GaaBVFT5zA4FlnK8]

Simply, verification answers the question {\it do we build things correctly}, validation answers the question {\it do we build the right thing}?

\seccc System description

Defining system requirements is the first and very important step for a successful and secure system or component. In this phase, it is important to define the limits, interfaces, functions, and the environment of the system.

It is important to note that SIL levels\fnote{Safety integrity level}, which we will discuss later, are always related to a component - e.g. a function. So we cannot say that the whole platform is a SIL, but only that the platform provides a service that has that SIL.

To illustrate, let's take a concrete example: automatic train control, specifically emergency braking in case of a dangerous situation, e.g. when a train runs a red light. In this case the {\sbf function} is: automatic braking when passing a dangerous signal. The driver's cab, the track equipment and the signal transmission path are the {\sbf system}. The {\sbf Interfaces} for us are the wheels, the dashboard which serves as an API for the driver to communicate with the system and the warning light which signals the entry prohibition.

\seccc Risk analysis

We now specify the gambles in this setting. The {\sbf hazard} standard defines it as any condition that can lead to an accident.\cite[GaaBVFT5zA4FlnK8]

In our example, we can consider a gamble as the system not initiating emergency braking or not receiving information to stop. In security, if there is a gamble, there is always a risk. The standard defines {\sbf risk} as the combination of the expected frequency of occurrence of the hazard and the expected severity of the hazard.

To make a system safe, our goal is to minimize risk to the point that it is equal to or less than the acceptable risk. We achieve this by adding just safe features to our system such as the redundancy principle, the watchdog principle, or the error detection principle.

Risk analysis consists of some of the methods already mentioned: hazard identification, frequency minimization, and consequence classification.

\seccc Acceptable risk

The European Union specifies in the CSM regulation\fnote{Common Safety Method for Risk Evaluation and Assessment} which methods can be used to calculate acceptable risk.

\begitems
* {\sbf Application of codes of practice}: The use of established norms, standards, and best practice methodologies for risk assessment and management that have been validated in practice.

* {\sbf Comparison with similar system}: Risk assessment based on analysis and comparison with other systems that have similar functionality, architecture, or operating conditions. This approach is particularly useful when assessing a new context.

* {\sbf Explicit risk estimation}: Direct and detailed quantitative or qualitative risk assessment using specific methods. This approach requires data collection and detailed modeling.\cite[ONTbnhbgf2hyhL9J]
\enditems

\seccc Hazard analysis (FTA, FMEA)

The standard {\it EN 50126} recommends the use of top-down analysis for hazard analysis. Specifically, {\sbf Fault Tree Analysis (FTA)} which allows us to work with dependencies between individual hazards. The second method used is {\sbf Failure Modes and Effects Analysis (FMEA)}. The FTA is suitable for analyzing multiple system failures; FMEA is suitable for analyzing a single failure, including all its consequences.

\seccc Tolerated risk ratios (THR, TFFR)

{\sbf Tolerable Hazard Rate (THR)}, which represents the maximum frequency of occurrence of a particular hazard. It therefore determines how many times a potentially hazardous event can occur in a defined period without affecting the overall security of the system.~\cite[vDBnVxdnHZs4vPl6]

{\sbf Tolerable Functional Failure Rate (TFFR)}, which represents the maximum frequency of failure of a specific system function, focuses on the system's functional aspects and considers how often a function can fail without leading to unacceptable risk.~\cite[vDBnVxdnHZs4vPl6]

THR will be sufficient if our system consists of only one safe function. However, if there is more than one, we need to use TFFR to combine them.

\seccc Safety Integrity Level (SIL)

The term SIL is a method used to evaluate and classify the level of integrity of the safety functions of a system. Previous methods have described to us how to deal primarily with random error. SIL tells us what type of architecture, validation, testing, etc. to choose depending on the level of system error.

\midinsert \clabel[TFFRxSIL]{Table describes TFFR and SIL relation}
\ctable{ll}{
\hfil {\sbf TFFR [$h^{-1}$] } & {\sbf SIL level} \crl
$10^{-9} \leq TFFR < 10^{-8}$ & 4 \cr
\cr
$10^{-8} \leq TFFR < 10^{-7}$ & 3 \cr
\cr
$10^{-7} \leq TFFR < 10^{-6}$ & 2 \cr
\cr
$10^{-6} \leq TFFR < 10^{-5}$ & 1 \cr
\cr
$10^{-5} \leq TFFR$ & basic integrity \cr
}
\caption/t Table describes TFFR and SIL relation.\cite[SU4ccYhZgv796Fi2]
\endinsert

The norm describes 4 Safety Integrity Levels from 1 to 4 and Basic Integrity. The SIL 4 is the highest level and the Basic Integrity is the lowest, as described in the table~\ref[TFFRxSIL].~\fnote{The norm {\it EN 50129} from year 2003 introduced term {\sbf SIL 0} to indicate non-safety-related functions. This terms in no longer used in the new norms version.}

\secc The five base questions

I would like to mention that an excellent mechanism to verify that we are we haven't forgotten anything elementary is to answer these five basic questions that based on {\it EN 50126}.

\begitems
* {\sbf What} - What features must be implemented?
* {\sbf How} - What steps must be followed during implementation?
* {\sbf With what} - What tools must be used for implementation?~\fnote{The tool problematic is detailed discussed int the chap \ref[ToolsClassification].}
* {\sbf Assurance} - How can I ensure that the first three questions are answered correctly?
* {\sbf Traceability} - Have I recorded everything in a way that allows for audit and verification?
\enditems


\sec Mechanisms to achieve SIL level

I would like to discuss the mechanisms, architecture types, principles, and policies that ensure that a given platform contains SIL functionality. The thesis does not address the processes behind the development of safety software, which are an integral part of the overall development process.

 
In this chapter we will use terms defined by {\it EN 50128} and {\it EN 50129}, which are widely used in their annexes and can be found in this thesis in tables or figures. In order to understand them, let's clarify them.

\begitems
* {\sbf `M` - Mandatory}: This symbol means that the use of a technique is mandatory.~\cite[vDBnVxdnHZs4vPl6]

* {\sbf `HR` - Highly Recommended}: This technique or measure is highly recommended for the SIL. It is considered a key measure, and implementation is mandatory. In case of absence, clear reasoning is required.

* {\sbf `R` - Recommended}: This technique or measure is recommended but not required or necessary to meet the safety requirement for a given SIL. Implementation of this technique can improve safety and reliability.

* {\sbf `-` No Recommendation}: There is no recommendation or non-recommendation for this technique or measure. Use is discretionary and should be decided based on the requirement specification.

* {\sbf `NR` - Not Recommended}: This technique or measure is not recommended for a given SIL. Implementation may be ineffective, inadequate, or even counterproductive.
\enditems

\secc The principles in developing high-integrity software

The {\it EN 50128} standard defines ten basic principles, the application of which in the development of high integrity software is not mandatory. Let's highlight the six most important ones.

\begitems
* {\sbf Top-down design method}: This method divides the system into smaller parts and gradually works its way from generalities to specific issues.

* {\sbf Modularity}: The modularity aims to divide software into smaller independent blocks that can be maintained and tested independently. This division often makes it possible to work on as many components in parallel as possible at the same time. In addition, it allows for easier testing or making changes. The disadvantage can be a certain necessary level of abstraction, which in the case of embedded systems can be undesirable.

* {\sbf Verification of each phase of the development life-cycle}: This method aims to minimize the risk of errors during development by thoroughly verifying each stage. This leads to the early identification of problems and therefore saves time and money.

* {\sbf Verified components and component libraries}: The goal is to reduce the risk of bugs in systems by reusing verified components. In addition to preventing errors, this measure also saves money. Since the validation process of developing new software is very demanding, it is therefore preferable to use components that are already validated.

* {\sbf Clear documentation and traceability}: The aim is to provide a clearly interpretable, understandable, and quickly comprehensible description of the component.

* {\sbf Auditable documents}: The aim is to provide evidence that the system is developed and tested in accordance with the requirements.
\enditems

\secc Architecture

In designing SIL software, the standard defines 7 basic principles. However, in relevance to the use of a microprocessor, they can be generalized to 3 basic principles or approaches to ensure that the code is safe and meets all requirements.\fnote{The sub-parts of these mechanisms are described in {\it EN 50129}, Annex A, Table E.4.} Each of these approaches has its advantages and limitations depending on the SIL level required.

\begitems
* {\sbf Inherent fail-safety}: This approach ensures the safety of the system due to the intrinsic properties of the design. Functions are designed to be fault tolerant without the need for external intervention. It is therefore more a matter of hardware design. This method cannot be implemented in software. As an example, the signal remains in a safe state even after a power failure, without external intervention. This mechanism is recommended (R) for {\it SIL 1} and {\it SIL 2}, and highly recommended (HR) for {\it SIL 3} and {\it SIL 4}.

* {\sbf Reactive fail-safety}: The approach focuses on responding to faults typically through fault detection and subsequent activation of protection mechanisms. The system is therefore able to react on its own without external intervention to ensure a safe state. Examples include activation of emergency braking or safety shutdown when a failure of the main system is detected. The mechanism is recommended (R) for {\it SIL 1} and {\it SIL 2} and highly recommended (HR) for {\it SIL 3} and {\it SIL 4}.

* {\sbf Composite fail-safety}: The approach combines multiple channels with fail-safe mechanisms for peer-to-peer comparison. This provides greater resilience to failure through redundancy and independence. An example would be the need to obtain a positive signal from at least two of the three independent units to activate an action element. This mechanism, like the inherent and reactive mechanisms, is recommended (R) for {\it SIL 1} and {\it SIL 2} and highly recommended (HR) for {\it SIL 3} and {\it SIL 4}.
\enditems

The standard also defines other architectural approaches that are less suitable for higher levels of safety integrity ({\it SIL 3} and {\it SIL 4}). These approaches can only be used in less critical applications such as {\it SIL 1} and {\it SIL 2}. These include, for example, a duplicated electronic structure, but where the channels may not be completely independent and the comparison of results may not be fail-safe. Another test is, for example, a simple electronic structure with self-tests, thus supervising its own functions. However, this approach again lacks independence between the function and its supervision. This limits this method, like the previous one, for higher SIL levels.

\midinsert \clabel[TechniquesSIL]{Table showing techniques and measures for different SIL levels}
\ctable{lllll}{
\hfil {\sbf Technique/Measure} & {\sbf SIL 1} & {\sbf SIL 2} & {\sbf SIL 3} & {\sbf SIL 4} \crl
Separation of safety-related functions from \cr
non-safety-related functions to prevent  & R & R & R & R \cr
unintended influences\cr
\cr
single electronic structure with self-tests & R & R & NR & NR \cr
and supervision\cr
\cr
single electronic structure based on & R & R & HR & HR \cr
inherent fail-safety\cr
\cr
single electronic structure based on reactive & R & R & HR & HR \cr
fail-safety \cr
\cr
Dual electronic structure & R & R & NR & NR \cr
\cr
Dual electronic structure based on\cr
composite fail-safety with fail-safe & R & R & HR & HR \cr
comparison\cr
\cr
Diverse electronic structure with fail-safe & R & R & HR & HR \cr
comparison \cr
}
\caption/t Table showing techniques and measures for different SIL levels according to {\it EN 50129}, Annex A, Table E.4~\cite[SU4ccYhZgv796Fi2]
\endinsert

Table \ref[TechniquesSIL] clearly shows the different mechanisms depending on the SIL level as defined by the standard. It is also not a bad idea to combine different mechanisms together. By definition, however, this cannot be implemented for all techniques.

\secc Software architecture technique

{\it EN 50128} in Annex A defines specific mechanisms to be used in the design of the software architecture and its implementation for different SIL levels. We have listed some of them in Table \ref[TechniquesMeasures].


\midinsert \clabel[TechniquesMeasures]{Software Architecture mechanism for different SIL levels}
\ctable{lllll}{
\hfil {\sbf Technique/Measure} & {\sbf SIL 1} & {\sbf SIL 2} & {\sbf SIL 3} & {\sbf SIL 4} \crl
Defensive Programming & HR & HR & HR & HR \cr
\cr
Fault Detection \& Diagnosis & R & R & HR & HR \cr
\cr
Error Detecting Codes & R & R & HR & HR \cr
\cr
Failure Assertion Programming & R & R & HR & HR \cr
\cr
Safety Bag Techniques & R & R & R & R \cr
\cr
Diverse Programming & R & R & HR & HR \cr
\cr
Recovery Block & R & R & R & R \cr
\cr
Backward Recovery & NR & NR & NR & NR \cr
\cr
Forward Recovery & NR & NR & NR & NR \cr
\cr
Retry Fault Recovery Mechanisms & R & R & R & R \cr
\cr
Software Error Effect Analysis & R & R & HR & HR \cr
\cr
Graceful Degradation & R & R & HR & HR \cr
\cr
Information Hiding & - & - & - & - \cr
\cr
Information Encapsulation & HR & HR & HR & HR \cr
\cr
Fully Defined Interface & HR & HR & M & M \cr
\cr
Formal Methods & R & R & HR & HR \cr
\cr
Modeling & R & R & HR & HR \cr
\cr
Structured Methodology & HR & HR & HR & HR \cr
\cr
Modeling supported by CAD and specification tools & R & R & HR & HR \cr
}
\caption/t Software Architecture mechanism for different SIL levels according to {\it EN 50128}, Annex A Table A.3.\cite[vDBnVxdnHZs4vPl6]
\endinsert

% TODO: discuess specific mechanism and describe it
\seccc Defensive Programming 
The {\sbf Defensive Programming} is one of the most important techniques in safety programming. The aim is to produce programs that detect anomalous control flow, data flow, or data values during their execution and react to these in a predetermined and acceptable manner.~\cite[vDBnVxdnHZs4vPl6]

There are three basic techniques of defensive programming:

\begitems
* All data\fnote{By data we mean all values stored in memory such as variables, objects, arrays, etc.} is important until proven otherwise.
* All input data is potentially hazardous until proven otherwise.
* All code is dangerous until proven otherwise.
\enditems

There exist many techniques of defensive programming. For example, to ensure that the numbers manipulated by the program are reasonable, the norm {\it EN 50128} recommends that:

\begitems
* variables should be range-checked;
* where possible, values should be checked for plausibility;~\fnote{Plausibility means quality of seeming likely to be true, or possible to believe.~\cite[6i2KXzgcdbBD85cyPlausibility]}
* parameters to procedures should be type, dimension, and range checked at procedure entry.
\enditems

Safe software should be designed to expect failures in its own environment. The norm {\it EN 5018} also defines three techniques:

\begitems
* Input variables and intermediate variables with physical significance should be checked for plausibility.
* The effect of output variables should be checked, preferably by direct observation of associated system state changes.
* The software should check its configuration. This could include both the existence and accessibility of expected hardware and also that the software itself is complete. This is particularly important for maintaining integrity after maintenance procedures.
\enditems

There are more techniques like reusing quality code, handling I/O, testing, low tolerance, canonization, or control flow sequence checking, but the CENELEC norms do not explicitly mention them.~\cite[vDBnVxdnHZs4vPl6, KFTfJxp6xrYSLALk, Blochc2008]

\seccc Fault Detection and Diagnosis

The goal of {\sbf Fault Detection and Diagnosis} is to detect faults in a system, which might lead to a failure, thus providing the basis for countermeasures in order to minimize the consequences of failure.~\cite[vDBnVxdnHZs4vPl6]

Fault detection is based on the principles of {\sbf redundancy} and {\sbf diversity}. The redundancy can detect hardware faults, and diversity can detect software faults. For correct results interpretation, it is necessary to use some voting system.

Special applicable methods for software level are {\sbf assertion programming}, {\sbf N-version programming},\fnote{The {\sbf N-version programming}, also known as a multiple-version dissimilar software, is the method where multiple functionally equivalent programs are independently generated from the same initial specifications.~\cite[532621]} or {\sbf safety bag technique}. For hardware level control loops, error checking codes, etc.


\seccc Error Detecting and Correcting Codes

The techniques {\sbf Error Detecting and Correcting Codes} aim to detect and correct errors in sensitive information.~\cite[vDBnVxdnHZs4vPl6]

To secure $n$ bits information is necessary to generate $k$ bits of block code. The block code can be generated by different methods, like: Hamming codes, cyclic codes, polynomial codes, hash codes, or cryptographic codes.

\seccc Failure Assertion Programming

The goal of {\sbf Failure Assertion Programming} is to detect residual faults during the execution of a software program.~\cite[vDBnVxdnHZs4vPl6]

\medskip
\clabel[FailureAssertionProgramming]{Failure Assertion Programming illustration}
\picw=14cm \cinspic img/02-failure-assertion-programming.png
\caption/f Failure Assertion Programming illustration.
\medskip

The assertion programming method in safety software development follows the idea of checking a {\sbf pre-condition}\fnote{The initial conditions are checked for validity before a sequence of statements is executed.} and a {\sbf post-condition}\fnote{Results are checked after t he execution of a sequence of statements.} as illustrated on Figure \ref[FailureAssertionProgramming]. If either the pre-condition or the post-condition is not met, the processing terminates with an error.

\seccc Safety Bag Techniques

The {\sbf Safety Bag} technique is aimed at protecting against residual specification and implementation faults in software that adversely affect safety.~\cite[vDBnVxdnHZs4vPl6]

Practically, it is an external monitoring system implemented on an independent computer with a separate specification. Its primary role is to ensure that the main computer performs safe actions, though not necessarily correct ones. The safety bag continuously monitors the main computer and prevents the system from entering an unsafe state. Additionally, if it detects that the main computer is approaching a potentially hazardous state, it must restore the system to a safe condition, either on its own or in coordination with the main computer.


\seccc Diverse Programming

The goal of {\sbf Diverse Programming} technique is to detect and mask residual software design faults during the execution of a program, in order to prevent safety-critical failures of the system, and to continue operation for high reliability.~\cite[vDBnVxdnHZs4vPl6]

The technique is based on relying on multiple implementations of the same specification. Consequently, we expect that if the inputs are the same for all implementations, the outputs will be the same. If this is not the case, we need to have a voting strategy that tells us whether we accept a given outcome and possibly which one.~\cite[inbookNVS]

\medskip
\clabel[DiverseProgramming]{Diverse Programming illustration}
\picw=14cm \cinspic img/02-NVS.png
\caption/f Diverse Programming illustration.
\medskip

This technique does not eliminate residual software design faults but provides a mechanism to detect and mitigate them before they impact safety.

Studies and experiments\fnote{Source is the norm {\it EN 50128}, Annex D, Chap D.16.} indicate that N-version programming does not always achieve the expected effectiveness. Despite using different algorithms, diverse software versions frequently fail on the same inputs.

\seccc Recovery Block

The {\sbf Recovery Block} is the technique to increase the likelihood of the program performing its intended function.~\cite[vDBnVxdnHZs4vPl6]

This technique may appear similar to Diverse Programming (N-version programming), but there are key differences between them. In the {\sbf N-version programming} technique, N independent groups or individual developers, who do not share the programming process, each develop a separate version of a software module. The underlying idea is that different developers will make different mistakes, thereby covering all possible faults. In the {\sbf Recovery Block technique}, different algorithms are assigned to distinct try blocks, which serve as redundant components. Unlike N-version programming, the redundant copies do not execute simultaneously. Instead, the result of each try block is evaluated using an acceptance test to determine its validity.~\cite[nBWFyWQg5kF6u8OP]

 
\seccc Retry Fault Recovery Mechanism

The goal of the {\sbf Re-Try Fault Recovery Mechanism} is to attempt functional recovery from a detected fault condition by re-try mechanism.~\cite[vDBnVxdnHZs4vPl6]

If an error is detected in a condition or procedure, re-execution of the same code can be started. In case of failure of a major part, a re-boot or re-start can be performed. In the case of using tasks and re-scheduling or re-starting the task.


\seccc Software Error Effect Analysis

The goal of the {\sbf SEEA (Software Error Effect Analysis)} is to identify software components and their criticality to propose means for detecting software errors and enhancing software robustness, and to evaluate the amount of validation needed on the various software components.~\cite[vDBnVxdnHZs4vPl6]


The norm {\it EN 50128} describes three phases of the SEEA, that is a powerful bug-finding method if it is carried out by an independent team:

\begitems
* {\sbf Vital software components identification} - This phase aims to determine the depth of the analysis needed for each software component, from its specification.
* {\sbf Software error analysis} - The second phase aims to provide the following information:
    \begitems \style 1
    * component name,
    * error considered,
    * consequences of the error at the module level,
    * consequences at the system level,
    * violated safety criterion,
    * error criticality,
    * proposed error detection means,
    * violated criterion if the detection means is implemented,
    * and residual criticality if the detection means is implemented.
    \enditems
* {\sbf Synthesis} - The third phase aims to highlight the remaining unsafe scenarios and determine the validation effort required based on the criticality of each module.
\enditems

\seccc Graceful Degradation

The technique {\sbf Graceful Degradation} aims to maintain the more critical system functions available despite failures by dropping the less critical functions.~\cite[vDBnVxdnHZs4vPl6]

The graceful degradation refers to the ability of a system to maintain a partial level of functionality when some components fail or are otherwise damaged. Instead of failing completely, a system designed with graceful degradation can reduce its quality of service while still providing essential functionality. This contrasts with "fail-stop" behavior, where the system completely ceases to function upon failure. The goal of graceful degradation is to ensure that users benefit from a reduced but functional level of service, thus minimizing the impact of failure. A system that is designed in this way is sometimes called fail-soft or fail-safe.~\cite[stallings2014, jNF0rhyXUI1gPjtr]

\seccc Information Encapsulation

The goal {\sbf Information Encapsulation} technique is to increase the robustness and maintainability of software.~\cite[vDBnVxdnHZs4vPl6]

Globally accessible data can be unintentionally or incorrectly modified by any software component, potentially requiring thorough code reviews and extensive changes. To mitigate these issues, information hiding is a widely used approach. It involves restricting direct access to key data structures, allowing them to be modified only through a predefined set of access procedures. This ensures that internal changes—such as altering data structures or adding new procedures—do not impact the overall functionality of the software. For instance, a directory system might include access procedures like $Insert$, $Delete$, and $Find$. These procedures, along with the underlying data structures, could be re-implemented (e.g., by adopting a different lookup method or storing data on a hard disk) without changing the logical behavior of the software that relies on them.

\seccc Fully Defined Interface

The technique {\sbf Fully Defined Interface} leads to modularization. A {\sbf Modular Approach} contains several rules for coding, maintenance phases, and design of the software project. The norm {\it EN 50128} defines a list of these rules to reach the modular methods for interfaces.

\begitems
* A component or module shall have a single well-defined task or function to fulfill.
* Connections between components or modules shall be limited and strictly defined; coherence in one component or module shall be strong.
* Collections of subprograms shall be built providing several levels of components or modules.
* Subprograms shall have a single entry and a single exit only.
* Components or modules shall communicate with other components or modules via their interfaces. Where global or common variables are used, they shall be well structured, access shall be controlled, and their use shall be justified in each instance.
* All components or module's interfaces shall be fully documented.
* Any component or module's interface shall contain the minimum number of parameters necessary for the component or module's function.
* A suitable restriction of parameter number shall be specified.~\fnote{The norms {\it EN 50128} defines that suitable number is typically 5.}
\enditems

\seccc Formal methods

The {\sbf Formal Methods} refer to mathematically rigorous techniques and tools for the specification, design, and verification of software and hardware systems. The application of formal methods in software and hardware design is driven by the idea that, similar to other engineering fields, conducting rigorous mathematical analysis enhances the reliability and robustness of a design.~\cite[vDBnVxdnHZs4vPl6, j9q6czqZVYZTTIDS]

There is no universal formal method that is suitable for all scenarios. Rather, an appropriate mathematical model must be chosen for each situation. Therefore, we have several models. The standard {\it EN 50128} describes CSP, CCS, HOL, LOTOS, OBJ, Temporal Logic, VDM, Z Method, B Method and Model Checking. Let's at least take a closer look at the most important ones.













\label[ToolsClassification]
\secc Tools classification

The norm {\it EN 50128}~\fnote{Specifically, clauses 3.1.42-44 in the aforementioned EN 50128.} defines three basic categories of tools used for safety software development.

\begitems
* {\sbf T1}: This class includes tools that do not produce any output that could directly or indirectly contribute to the resulting executable software code. An example of a tool is a text editor that has no ability to generate code.
* {\sbf T2}: This class includes tools that support testing or verification of the design or executable code. Errors in a tool may occur in certain situations, but cannot directly cause errors in the resulting executable software. Examples of such tools include static analysis tools or tools for measuring test coverage.
* {\sbf T3}: This class includes tools that generate outputs that directly or indirectly contribute to the final executable code of the resulting system. These include, for example, compilers.
\enditems

It is important to note that depending on the class of a given tool, requirements are made on them, which are checked during validation and verification. Therefore, it is important to keep them in mind when designing a system. Often it may be more appropriate for a project to use an older but validated tool for which its behavior has already been proven and de-validated.


\secc Programming technique

The standards also define the programming standards for the individual SIL levels. The individual techniques are shown in table \ref[TechniquesCoding].

\midinsert \clabel[TechniquesCoding]{Coding standards for different SIL levels}
\ctable{lllll}{
\hfil {\sbf Technique/Measure} & {\sbf SIL 1} & {\sbf SIL 2} & {\sbf SIL 3} & {\sbf SIL 4} \crl
Coding Standard & HR & HR & M & M \cr
\cr
Coding Style Guide & HR & HR & HR & HR \cr
\cr
No Dynamic Objects & R & R & HR & HR \cr
\cr
No Dynamic Variables & R & R & HR & HR \cr
\cr
Limited Use of Pointers & R & R & R & R \cr
\cr
Limited Use of Recursion & R & R & HR & HR \cr
\cr
No Unconditional Jumps & HR & HR & HR & HR \cr
\cr
Limited Size and Complexity & HR & HR & HR & HR \cr
of Functions, Subroutines, and Methods \cr
\cr
Entry/Exit Point strategy for Functions & HR & HR & HR & HR \cr
Subroutines, and Methods\cr
\cr
Limited Number of subroutine parameters & R & R & R & R \cr
\cr
Limited Use of Global Variables & HR & HR & M & M \cr
}
\caption/t Coding standards for different SIL levels according to {\it EN 50128}, Annex A, Table A.12.~\cite[vDBnVxdnHZs4vPl6]
\endinsert


Compared to other types of software, the development of the safe one differs mainly in techniques such as {\sbf not recommending the use of dynamic memory allocation}, both for objects and variables. This is due to the greater possibility of failure, as the software allocates memory at runtime, and this can lead to failures. The standard recommends {\sbf limited use of recursion and pointers}. These are methods that can potentially mishandle memory - either filling it up quickly or being ill-defined. Therefore, unless necessary, it is not recommended to use them.

Specific techniques for safe development are also {\sbf limited size and complexity of functions, subroutines and methods} and {\sbf input or output strategies for functions, subroutines and methods}.

The other techniques are not, in my view, specific to secure development, but are principles that are appropriate for any software development, such as following the {\sbf coding standard}, the {\sbf coding style guide} that leads to improved readability, or the {\sbf limited use of global variables}.

It may also be interesting to note that the standard defines {\sbf recommended programming languages} for different SIL levels.\fnote{The EN 50128 standard describes the different programming languages in Table A.15 in Annex A} It will come as no surprise that C, C++ or Assembler are among the recommended languages. However, the presence of languages such as C\# or Java may also come as a surprise. The standard also mentions more historical languages such as Pascal.